{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e246693",
   "metadata": {},
   "source": [
    "Data provided hold the historical records of Opportunities (a potential deal with client). Opportunity is presented by the following (main) columns (properties): OpportunityId - to identify a separate Opportunity uniquely, CreatedDate - the date of the record creation, StageName - probability and the short stage description for a successful deal closing (100% Opportunity) Amount - (a current corrected version of) sum of the (potential) deal // ??? CloseDate - forecasted deal closing date Probability - the probability of success for this opportunity at its current stage other fields - properties of the Opportunity\n",
    "\n",
    "You have the history of interactions with clients (Opportunity is a potential deal).\n",
    "You should forecast the Probability of successful deal (Opportunity) closing.\n",
    "Speaking more precisely, to forecast most likely next Probability using the known previous history for this Opportunity (potential deal), namely Stage + Probability changes in previous time.\n",
    "Also you could use available properties of that Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380bf5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f08620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bohdan Potuzhnyi\\AppData\\Local\\Temp\\ipykernel_26792\\1806839911.py:1: DtypeWarning: Columns (14,32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"train_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"train_data.csv\")\n",
    "dfe = pd.read_csv(\"test_data_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf7b803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Unnamed: 0                             0\nId                                     0\nOpportunityId                          0\nCreatedById                            0\nCreatedDate                            0\nCreatedDateForInsert                   0\nStageName                              0\nAmount                                 0\nProbability                            0\nPrevOpportunityStageUpdate             0\nValidThroughDate                    6954\nSystemModstamp                         0\nIsDeleted                              0\nPrevAmount                         28705\nPrevCloseDate                      27916\nAccountId                              0\nRecordTypeId                        1704\nStageSortOrder                         0\nType                                   3\nLeadSource                          5536\nCampaignId                             0\nOwnerId                                0\nTerritory2Id                           0\nFiscalYear                             0\nFiscalQuarter                          0\nContactId                              0\nPartnerAccountId                     626\nNumber_of_Seats__c                  3220\nSCC_Interest__c                        0\nCompany_s_CRM_Type__c               8293\nDeal_type__c                        8625\nNeeds__c                           22493\nType_of_Users__c                   32616\nTAM__c                              1873\nSecondary_CSM__c                    2238\nSource_Type__c                     10396\nOriginal_Lead_Source__c             7715\nOriginal_Lead_Source_Details__c    10221\ndtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_count = df.isnull().sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a5874d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bohdan Potuzhnyi\\AppData\\Local\\Temp\\ipykernel_26792\\3667763167.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  datae = dfe[df['StageName'].str.contains('100%') | df['StageName'].str.contains('0% prob')]\n",
      "C:\\Users\\Bohdan Potuzhnyi\\AppData\\Local\\Temp\\ipykernel_26792\\3667763167.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.StageName = data.StageName.apply(lambda x: '1' if '100' in x else '0')\n",
      "C:\\Users\\Bohdan Potuzhnyi\\AppData\\Local\\Temp\\ipykernel_26792\\3667763167.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  datae.StageName = datae.StageName.apply(lambda x: '1' if '100' in x else '0')\n"
     ]
    }
   ],
   "source": [
    "data = df[df['StageName'].str.contains('100%') | df['StageName'].str.contains('0% prob')]\n",
    "datae = dfe[df['StageName'].str.contains('100%') | df['StageName'].str.contains('0% prob')]\n",
    "\n",
    "data.StageName = data.StageName.apply(lambda x: '1' if '100' in x else '0')\n",
    "datae.StageName = datae.StageName.apply(lambda x: '1' if '100' in x else '0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37927834",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "series = data.groupby(\"OpportunityId\").OpportunityId.count()\n",
    "series.sort_values(inplace=True)\n",
    "series.head(3000)\n",
    "series.value_counts().plot(kind = 'barh')\n",
    "#opportunity_all = series[\"OpportunityId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "print(series[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "894415b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_opportunities = []\n",
    "opp_longest = series[-1]\n",
    "\n",
    "all_opor_e = []\n",
    "\n",
    "#used to fulfill np.array to be with max length\n",
    "example_arr = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    #print(row)\n",
    "    all_opportunities.append(row[\"OpportunityId\"])\n",
    "all_opportunities = np.unique(all_opportunities)\n",
    "\n",
    "for index, row in datae.iterrows():\n",
    "    all_opor_e.append(row[\"OpportunityId\"])\n",
    "all_opor_e = np.unique(all_opor_e)\n",
    "#print(all_opportunities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "6\n",
      "47\n",
      "4\n",
      "18\n",
      "10\n",
      "3\n",
      "14\n",
      "6\n",
      "20\n",
      "10\n",
      "59\n",
      "[1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#for each opportunity and not id\n",
    "#def\n",
    "#here\n",
    "#\n",
    "#[license, service, upgrade, renewal, oem, global, new customer]\n",
    "type_array = [0, 0, 0, 0, 0, 0, 0]\n",
    "############\n",
    "def create_dict(df, column):\n",
    "    index_dict = {}\n",
    "\n",
    "    all_unique_values = df[column].unique()\n",
    "    tmp = []\n",
    "\n",
    "    for el in all_unique_values:\n",
    "        if el is not np.nan:\n",
    "            tmp.append(el)\n",
    "\n",
    "    all_unique_values = tmp\n",
    "\n",
    "    for i in range(0, len(all_unique_values)):\n",
    "           index_dict[all_unique_values[i]] = i\n",
    "\n",
    "    return index_dict\n",
    "\n",
    "\n",
    "def flag_arr(dict, str):\n",
    "    arr = [0] * len(dict)\n",
    "\n",
    "    if str in dict:\n",
    "        arr[dict[str]] = 1\n",
    "    arr = np.array(arr)\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "types_to_index = create_dict(data, \"Type\")\n",
    "lead_source_to_index = create_dict(data, \"LeadSource\")\n",
    "owner_id_to_index = create_dict(data, \"OwnerId\")\n",
    "fiscal_quarter_to_index = create_dict(data, \"FiscalQuarter\")\n",
    "fiscal_year_to_index = create_dict(data, \"FiscalYear\")\n",
    "company_s_crm_typec_to_index = create_dict(data, \"Company_s_CRM_Type__c\")\n",
    "deal_typec_to_index = create_dict(data, \"Deal_type__c\")\n",
    "source_typec_to_index = create_dict(data, \"Source_Type__c\")\n",
    "original_lead_sourcec_to_index = create_dict(data, \"Original_Lead_Source__c\")\n",
    "original_lead_source_details__c_to_index = create_dict(data, \"Original_Lead_Source_Details__c\")\n",
    "stage_sort_order = create_dict(data, \"StageSortOrder\")\n",
    "created_by_id = create_dict(data, \"CreatedById\")\n",
    "\n",
    "print(len(list(types_to_index)))\n",
    "print(len(list(lead_source_to_index)))\n",
    "print(len(list(owner_id_to_index)))\n",
    "print(len(list(fiscal_quarter_to_index)))\n",
    "print(len(list(fiscal_year_to_index)))\n",
    "print(len(list(company_s_crm_typec_to_index)))\n",
    "print(len(list(deal_typec_to_index)))\n",
    "print(len(list(source_typec_to_index)))\n",
    "print(len(list(original_lead_sourcec_to_index)))\n",
    "print(len(list(original_lead_source_details__c_to_index)))\n",
    "print(len(list(stage_sort_order)))\n",
    "print(len(list(created_by_id)))\n",
    "print(flag_arr(types_to_index, \"Licenses\"))\n",
    "############\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97947a27",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train completed\n"
     ]
    }
   ],
   "source": [
    "all_opportunities_array = []\n",
    "final_probability_array = []\n",
    "\n",
    "for opportunity in all_opportunities:\n",
    "    opp_id = opportunity\n",
    "    specific_opportinity = data.loc[(data.OpportunityId == opp_id)]\n",
    "\n",
    "    specific_opportinity = specific_opportinity.sort_values(by=\"CreatedDateForInsert\", ascending=True)\n",
    "\n",
    "    start_date = specific_opportinity.loc[specific_opportinity.index[0], \"CreatedDate\"]\n",
    "    start_date = datetime.strptime(start_date, '%m/%d/%Y %H:%M')\n",
    "\n",
    "    val_arr = []\n",
    "    val_ch = []\n",
    "    res_arr = []\n",
    "    time_filler = []\n",
    "    arr = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    for current in specific_opportinity[\"CreatedDateForInsert\"]:\n",
    "        current_date = datetime.strptime(current, '%m/%d/%Y %H:%M')\n",
    "        res_arr.append(int((current_date - start_date).days))\n",
    "        if j == 0:\n",
    "            arr.append(int((current_date - start_date).days))\n",
    "            j+=1\n",
    "        else:\n",
    "            arr.append(int((current_date - start_date).days) - arr[j-1])\n",
    "            j+=1\n",
    "\n",
    "        spc = len(specific_opportinity.index)\n",
    "\n",
    "        time_filler = np.ones(spc) * 0.0\n",
    "        arr_temp = np.append(arr,time_filler)\n",
    "        arr_temp = arr_temp[:spc]\n",
    "\n",
    "        res_arr_temp = np.append(res_arr,time_filler)\n",
    "        res_arr_temp = res_arr_temp[:spc]\n",
    "\n",
    "        val_arr.append(specific_opportinity.loc[specific_opportinity.index[i], \"Amount\"])\n",
    "        #print(val_arr)\n",
    "        if(i != 0):\n",
    "            val_ch.append((val_arr[i] - val_arr[i-1]))\n",
    "        else:\n",
    "            val_ch.append(0)\n",
    "        val_ch_temp = np.append(val_ch, time_filler)\n",
    "        val_ch_temp = val_ch_temp[:spc]\n",
    "\n",
    "        sp_save = specific_opportinity\n",
    "\n",
    "        specific_opportinity1 = specific_opportinity.loc[:, [\"Probability\"]]\n",
    "        specific_opportinity1.insert(1, \"Duration\", res_arr_temp)\n",
    "        specific_opportinity1.insert(2, \"DeltaDuration\", arr_temp)\n",
    "        specific_opportinity1.insert(3, \"DeltaAmount\", val_ch_temp)\n",
    "\n",
    "        final_probability = specific_opportinity1.iloc[i, specific_opportinity1.columns.get_loc('Probability')]\n",
    "        final_probability_array.append(final_probability)\n",
    "        #print(final_probability)\n",
    "        opp_array = np.array(specific_opportinity1.iloc[:i+1]).flatten()\n",
    "        #print(opp_array)\n",
    "        #l = input()\n",
    "        padding = np.ones(80) * 0.0\n",
    "        op_ar_new = np.append(padding,opp_array)\n",
    "        opp_array = op_ar_new[-80:]\n",
    "        #print(opp_array)\n",
    "        ### PART FOR INFO ADD ###\n",
    "        opp_array = np.append(opp_array,val_arr[i])\n",
    "        ###\n",
    "        ### PART FOR DICTIONARY CHEKS ###\n",
    "        opp_array = np.append(opp_array, flag_arr(types_to_index, sp_save.loc[sp_save.index[i], \"Type\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(lead_source_to_index, sp_save.loc[sp_save.index[i], \"LeadSource\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(owner_id_to_index, sp_save.loc[sp_save.index[i], \"OwnerId\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(fiscal_quarter_to_index, sp_save.loc[sp_save.index[i], \"FiscalQuarter\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(fiscal_year_to_index, sp_save.loc[sp_save.index[i], \"FiscalYear\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(company_s_crm_typec_to_index, sp_save.loc[sp_save.index[i], \"Company_s_CRM_Type__c\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(deal_typec_to_index, sp_save.loc[sp_save.index[i], \"Deal_type__c\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(source_typec_to_index, sp_save.loc[sp_save.index[i], \"Source_Type__c\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(original_lead_sourcec_to_index, sp_save.loc[sp_save.index[i], \"Original_Lead_Source__c\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(original_lead_source_details__c_to_index, sp_save.loc[sp_save.index[i], \"Original_Lead_Source_Details__c\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(stage_sort_order, sp_save.loc[sp_save.index[i], \"StageSortOrder\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(created_by_id, sp_save.loc[sp_save.index[i], \"CreatedById\"]))\n",
    "        #print(opp_array)\n",
    "        ###\n",
    "        all_opportunities_array.append(opp_array)\n",
    "        i+=1\n",
    "\n",
    "#size = 286\n",
    "\n",
    "print(\"Train completed\")\n",
    "\n",
    "all_opp_e_f = []\n",
    "fl_prob_e_f = []\n",
    "# doredachit\n",
    "for opportunity in all_opor_e:\n",
    "    opp_id = opportunity\n",
    "    specific_opportinity = datae.loc[(datae.OpportunityId == opp_id)]\n",
    "\n",
    "    specific_opportinity = specific_opportinity.sort_values(by=\"CreatedDateForInsert\", ascending=True)\n",
    "\n",
    "    start_date = specific_opportinity.loc[specific_opportinity.index[0], \"CreatedDate\"]\n",
    "    start_date = datetime.strptime(start_date, '%m/%d/%Y %H:%M')\n",
    "\n",
    "    val_arr = []\n",
    "    val_ch = []\n",
    "    res_arr = []\n",
    "    time_filler = []\n",
    "    arr = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    for current in specific_opportinity[\"CreatedDateForInsert\"]:\n",
    "        current_date = datetime.strptime(current, '%Y-%m-%d %H:%M:%S')\n",
    "        res_arr.append(int((current_date - start_date).days))\n",
    "        if j == 0:\n",
    "            arr.append(int((current_date - start_date).days))\n",
    "            j+=1\n",
    "        else:\n",
    "            arr.append(int((current_date - start_date).days) - arr[j-1])\n",
    "            j+=1\n",
    "\n",
    "        spc = len(specific_opportinity.index)\n",
    "\n",
    "        time_filler = np.ones(spc) * 0.0\n",
    "        arr_temp = np.append(arr,time_filler)\n",
    "        arr_temp = arr_temp[:spc]\n",
    "\n",
    "        res_arr_temp = np.append(res_arr,time_filler)\n",
    "        res_arr_temp = res_arr_temp[:spc]\n",
    "\n",
    "        val_arr.append(specific_opportinity.loc[specific_opportinity.index[i], \"Amount\"])\n",
    "        #print(val_arr)\n",
    "        if(i != 0):\n",
    "            val_ch.append((val_arr[i] - val_arr[i-1]))\n",
    "        else:\n",
    "            val_ch.append(0)\n",
    "        val_ch_temp = np.append(val_ch, time_filler)\n",
    "        val_ch_temp = val_ch_temp[:spc]\n",
    "\n",
    "        sp_save = specific_opportinity\n",
    "\n",
    "        specific_opportinity1 = specific_opportinity.loc[:, [\"Probability\"]]\n",
    "        specific_opportinity1.insert(1, \"Duration\", res_arr_temp)\n",
    "        specific_opportinity1.insert(2, \"DeltaDuration\", arr_temp)\n",
    "        specific_opportinity1.insert(3, \"DeltaAmount\", val_ch_temp)\n",
    "\n",
    "        final_probability = specific_opportinity1.iloc[i, specific_opportinity1.columns.get_loc('Probability')]\n",
    "        fl_prob_e_f.append(final_probability)\n",
    "        #print(final_probability)\n",
    "        opp_array = np.array(specific_opportinity1.iloc[:i+1]).flatten()\n",
    "        #print(opp_array)\n",
    "        #l = input()\n",
    "        padding = np.ones(80) * 0.0\n",
    "        op_ar_new = np.append(padding,opp_array)\n",
    "        opp_array = op_ar_new[-80:]\n",
    "        #print(opp_array)\n",
    "        ### PART FOR INFO ADD ###\n",
    "        opp_array = np.append(opp_array,val_arr[i])\n",
    "        ###\n",
    "        ### PART FOR DICTIONARY CHEKS ###\n",
    "        opp_array = np.append(opp_array, flag_arr(types_to_index, sp_save.loc[sp_save.index[i], \"Type\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(lead_source_to_index, sp_save.loc[sp_save.index[i], \"LeadSource\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(owner_id_to_index, sp_save.loc[sp_save.index[i], \"OwnerId\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(fiscal_quarter_to_index, sp_save.loc[sp_save.index[i], \"FiscalQuarter\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(fiscal_year_to_index, sp_save.loc[sp_save.index[i], \"FiscalYear\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(company_s_crm_typec_to_index, sp_save.loc[sp_save.index[i], \"Company_s_CRM_Type__c\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(deal_typec_to_index, sp_save.loc[sp_save.index[i], \"Deal_type__c\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(source_typec_to_index, sp_save.loc[sp_save.index[i], \"Source_Type__c\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(original_lead_sourcec_to_index, sp_save.loc[sp_save.index[i], \"Original_Lead_Source__c\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(original_lead_source_details__c_to_index, sp_save.loc[sp_save.index[i], \"Original_Lead_Source_Details__c\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(stage_sort_order, sp_save.loc[sp_save.index[i], \"StageSortOrder\"]))\n",
    "        opp_array = np.append(opp_array, flag_arr(created_by_id, sp_save.loc[sp_save.index[i], \"CreatedById\"]))\n",
    "        #print(opp_array)\n",
    "        ###\n",
    "        all_opp_e_f.append(opp_array)\n",
    "        i+=1\n",
    "\n",
    "#size = 286\n",
    "# for opportunity in all_opor_e:\n",
    "#     opp_id = opportunity\n",
    "#     specific_opportinity = datae.loc[(datae.OpportunityId == opp_id)]\n",
    "#\n",
    "#     specific_opportinity = specific_opportinity.sort_values(by=\"CreatedDateForInsert\", ascending=True)\n",
    "#\n",
    "#     start_date = specific_opportinity.loc[specific_opportinity.index[0], \"CreatedDate\"]\n",
    "#     start_date = datetime.strptime(start_date, '%m/%d/%Y %H:%M')\n",
    "#\n",
    "#     res_arr = []\n",
    "#     arr = []\n",
    "#     i = 0\n",
    "#     j = 0\n",
    "#     for current in specific_opportinity[\"CreatedDateForInsert\"]:\n",
    "#         current_date = datetime.strptime(current, '%Y-%m-%d %H:%M:%S')\n",
    "#         res_arr.append(int((current_date - start_date).days))\n",
    "#         if j == 0:\n",
    "#             arr.append(int((current_date - start_date).days))\n",
    "#             j+=1\n",
    "#         else:\n",
    "#             arr.append(int((current_date - start_date).days) - arr[j-1])\n",
    "#             j+=1\n",
    "#\n",
    "#     specific_opportinity = specific_opportinity.loc[:, [\"Amount\", \"Probability\"]]\n",
    "#     specific_opportinity.insert(2, \"Duration\", res_arr)\n",
    "#     specific_opportinity.insert(3, \"DeltaDuration\", arr)\n",
    "#     final_probability = specific_opportinity.iloc[-1, specific_opportinity.columns.get_loc('Probability')]\n",
    "#     fl_prob_e_f.append(final_probability)\n",
    "#\n",
    "#     opp_array = np.array(specific_opportinity.iloc[-20:]).flatten()\n",
    "#\n",
    "#     padding = np.ones(100) * 0.0\n",
    "#     op_ar_new = np.append(padding,opp_array)\n",
    "#     opp_array = op_ar_new[-80:]\n",
    "#     all_opp_e_f.append(opp_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "all_opportunities_array1 = np.asarray(all_opportunities_array)\n",
    "final_probability_array1 = np.asarray(final_probability_array)\n",
    "\n",
    "all_opor_e_f1 = np.asarray(all_opp_e_f)\n",
    "fl_prob_e_f1 = np.asarray(fl_prob_e_f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(all_opportunities_array1)\n",
    "print(all_opor_e_f1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 217)]             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                13952     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 101)               6565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,677\n",
      "Trainable params: 24,677\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def base_model():\n",
    "  inputs = tf.keras.Input(shape=(286,))\n",
    "  x = tf.keras.layers.Dense(128, activation='relu')(inputs)\n",
    "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "  outputs = tf.keras.layers.Dense(101, activation='softmax')(x)\n",
    "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "  return model\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "model = base_model()\n",
    "model.compile(optimizer=optimizer, loss=loss_object, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def train_step(x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    pred = model(x)\n",
    "    loss = model.compiled_loss(y, pred)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  return tf.linalg.global_norm(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "\n",
    "def create_generator(list_of_arrays):\n",
    "    for i in list_of_arrays:\n",
    "        yield i\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "dataset_opport = tf.data.Dataset.from_generator(lambda: create_generator(all_opportunities_array1),output_types= tf.int32, output_shapes=(286,))\n",
    "dataset_marks = tf.data.Dataset.from_generator(lambda: final_probability_array1,output_types= tf.int32, output_shapes=())\n",
    "\n",
    "dataset_test27 = tf.data.Dataset.zip((dataset_opport, dataset_marks)).batch(1)\n",
    "\n",
    "dataset_opport_e = tf.data.Dataset.from_generator(lambda: create_generator(all_opor_e_f1),output_types= tf.int32, output_shapes=(286,))\n",
    "dataset_marks_e = tf.data.Dataset.from_generator(lambda: fl_prob_e_f1,output_types= tf.int32, output_shapes=())\n",
    "\n",
    "dataset_test27_e = tf.data.Dataset.zip((dataset_opport_e, dataset_marks_e)).batch(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 217), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 217), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_test27)\n",
    "print(dataset_test27_e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30493\n",
      "4603\n",
      "tf.Tensor(\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0 100   0   0   0   0   1   0   0   0   0   0   0   0   1\n",
      "    0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "    0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   1\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]], shape=(1, 217), dtype=int32)\n",
      "tf.Tensor([100], shape=(1,), dtype=int32)\n",
      "<TakeDataset element_spec=(TensorSpec(shape=(None, 217), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
      "<TakeDataset element_spec=(TensorSpec(shape=(None, 217), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(len(list(dataset_test27)))\n",
    "print(len(list(dataset_test27_e)))\n",
    "\n",
    "for x,y in dataset_test27.take(1):\n",
    "  print(x)\n",
    "  print(y)\n",
    "\n",
    "print(dataset_test27.take(1))\n",
    "print(dataset_test27_e.take(1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30493it [05:01, 100.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4603/4603 [==============================] - 5s 1ms/step - loss: 2.9672 - root_mean_squared_error: 57.6329\n",
      "validation loss = 2.9672017097473145, accuracy = 57.632896423339844:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2047it [00:19, 106.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [29]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epoch_num):\n\u001B[0;32m      5\u001B[0m   \u001B[38;5;28;01mfor\u001B[39;00m step, (x,y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(tqdm(dataset_test27)):\n\u001B[1;32m----> 6\u001B[0m     grad_norm\u001B[38;5;241m.\u001B[39mappend(\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m      8\u001B[0m   loss, acc \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate(dataset_test27_e)\n\u001B[0;32m      9\u001B[0m   \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation loss = \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, accuracy = \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(loss, acc))\n",
      "Input \u001B[1;32mIn [25]\u001B[0m, in \u001B[0;36mtrain_step\u001B[1;34m(x, y)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m tape:\n\u001B[0;32m      3\u001B[0m   pred \u001B[38;5;241m=\u001B[39m model(x)\n\u001B[1;32m----> 4\u001B[0m   loss \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompiled_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m   gradients \u001B[38;5;241m=\u001B[39m tape\u001B[38;5;241m.\u001B[39mgradient(loss, model\u001B[38;5;241m.\u001B[39mtrainable_variables)\n\u001B[0;32m      6\u001B[0m   model\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mapply_gradients(\u001B[38;5;28mzip\u001B[39m(gradients, model\u001B[38;5;241m.\u001B[39mtrainable_variables))\n",
      "File \u001B[1;32mD:\\dev_programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py:239\u001B[0m, in \u001B[0;36mLossesContainer.__call__\u001B[1;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001B[0m\n\u001B[0;32m    236\u001B[0m loss_metric_values \u001B[38;5;241m=\u001B[39m losses_utils\u001B[38;5;241m.\u001B[39mcast_losses_to_common_dtype(\n\u001B[0;32m    237\u001B[0m     loss_metric_values)\n\u001B[0;32m    238\u001B[0m total_loss_metric_value \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39madd_n(loss_metric_values)\n\u001B[1;32m--> 239\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_loss_metric\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_state\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    240\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtotal_loss_metric_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_dim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    242\u001B[0m loss_values \u001B[38;5;241m=\u001B[39m losses_utils\u001B[38;5;241m.\u001B[39mcast_losses_to_common_dtype(loss_values)\n\u001B[0;32m    243\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39madd_n(loss_values)\n",
      "File \u001B[1;32mD:\\dev_programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\metrics_utils.py:70\u001B[0m, in \u001B[0;36mupdate_state_wrapper.<locals>.decorated\u001B[1;34m(metric_obj, *args, **kwargs)\u001B[0m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     65\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTrying to run metric.update_state in replica context when \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     66\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthe metric was not created in TPUStrategy scope. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     67\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMake sure the keras Metric is created in TPUstrategy scope. \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf_utils\u001B[38;5;241m.\u001B[39mgraph_context_for_symbolic_tensors(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 70\u001B[0m   update_op \u001B[38;5;241m=\u001B[39m update_state_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m update_op \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# update_op will be None in eager execution.\u001B[39;00m\n\u001B[0;32m     72\u001B[0m   metric_obj\u001B[38;5;241m.\u001B[39madd_update(update_op)\n",
      "File \u001B[1;32mD:\\dev_programs\\Python\\Python310\\lib\\site-packages\\keras\\metrics.py:178\u001B[0m, in \u001B[0;36mMetric.__new__.<locals>.update_state_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    176\u001B[0m control_status \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39m__internal__\u001B[38;5;241m.\u001B[39mautograph\u001B[38;5;241m.\u001B[39mcontrol_status_ctx()\n\u001B[0;32m    177\u001B[0m ag_update_state \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39m__internal__\u001B[38;5;241m.\u001B[39mautograph\u001B[38;5;241m.\u001B[39mtf_convert(obj_update_state, control_status)\n\u001B[1;32m--> 178\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ag_update_state(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\dev_programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:689\u001B[0m, in \u001B[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    687\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    688\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m conversion_ctx:\n\u001B[1;32m--> 689\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    690\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[0;32m    691\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[1;32mD:\\dev_programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:331\u001B[0m, in \u001B[0;36mconverted_call\u001B[1;34m(f, args, kwargs, caller_fn_scope, options)\u001B[0m\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m conversion\u001B[38;5;241m.\u001B[39mis_in_allowlist_cache(f, options):\n\u001B[0;32m    330\u001B[0m   logging\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAllowlisted \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: from cache\u001B[39m\u001B[38;5;124m'\u001B[39m, f)\n\u001B[1;32m--> 331\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_call_unconverted\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ag_ctx\u001B[38;5;241m.\u001B[39mcontrol_status_ctx()\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m==\u001B[39m ag_ctx\u001B[38;5;241m.\u001B[39mStatus\u001B[38;5;241m.\u001B[39mDISABLED:\n\u001B[0;32m    334\u001B[0m   logging\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAllowlisted: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: AutoGraph is disabled in context\u001B[39m\u001B[38;5;124m'\u001B[39m, f)\n",
      "File \u001B[1;32mD:\\dev_programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458\u001B[0m, in \u001B[0;36m_call_unconverted\u001B[1;34m(f, args, kwargs, options, update_cache)\u001B[0m\n\u001B[0;32m    455\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__self__\u001B[39m\u001B[38;5;241m.\u001B[39mcall(args, kwargs)\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 458\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs)\n",
      "File \u001B[1;32mD:\\dev_programs\\Python\\Python310\\lib\\site-packages\\keras\\metrics.py:449\u001B[0m, in \u001B[0;36mReduce.update_state\u001B[1;34m(self, values, sample_weight)\u001B[0m\n\u001B[0;32m    447\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg)\n\u001B[0;32m    448\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 449\u001B[0m   sample_weight \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    450\u001B[0m   \u001B[38;5;66;03m# Update dimensions of weights to match with values if possible.\u001B[39;00m\n\u001B[0;32m    451\u001B[0m   values, _, sample_weight \u001B[38;5;241m=\u001B[39m losses_utils\u001B[38;5;241m.\u001B[39msqueeze_or_expand_dimensions(\n\u001B[0;32m    452\u001B[0m       values, sample_weight\u001B[38;5;241m=\u001B[39msample_weight)\n",
      "File \u001B[1;32mD:\\dev_programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\dev_programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001B[0m, in \u001B[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1080\u001B[0m \u001B[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001B[39;00m\n\u001B[0;32m   1081\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1082\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m dispatch_target(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1083\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[0;32m   1084\u001B[0m   \u001B[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[39;00m\n\u001B[0;32m   1085\u001B[0m   \u001B[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001B[39;00m\n\u001B[0;32m   1086\u001B[0m   result \u001B[38;5;241m=\u001B[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001B[1;32mD:\\dev_programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1002\u001B[0m, in \u001B[0;36mcast\u001B[1;34m(x, dtype, name)\u001B[0m\n\u001B[0;32m   1000\u001B[0m   x \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mconvert_to_tensor(x, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1001\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mbase_dtype \u001B[38;5;241m!=\u001B[39m base_type:\n\u001B[1;32m-> 1002\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mgen_math_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbase_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1003\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mis_complex \u001B[38;5;129;01mand\u001B[39;00m base_type\u001B[38;5;241m.\u001B[39mis_floating:\n\u001B[0;32m   1004\u001B[0m   logging\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCasting complex to real discards imaginary part.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\dev_programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:1996\u001B[0m, in \u001B[0;36mcast\u001B[1;34m(x, DstT, Truncate, name)\u001B[0m\n\u001B[0;32m   1994\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tld\u001B[38;5;241m.\u001B[39mis_eager:\n\u001B[0;32m   1995\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1996\u001B[0m     _result \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_FastPathExecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1997\u001B[0m \u001B[43m      \u001B[49m\u001B[43m_ctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mCast\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mDstT\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDstT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTruncate\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTruncate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1998\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[0;32m   1999\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epoch_num = 5\n",
    "grad_norm = []\n",
    "for epoch in range(epoch_num):\n",
    "  for step, (x,y) in enumerate(tqdm(dataset_test27)):\n",
    "    grad_norm.append(train_step(x,y))\n",
    "\n",
    "  loss, acc = model.evaluate(dataset_test27_e)\n",
    "  print(\"validation loss = {}, accuracy = {}:\".format(loss, acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams['ytick.labelleft'] = True\n",
    "plt.plot(grad_norm)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}