{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e246693",
   "metadata": {},
   "source": [
    "Data provided hold the historical records of Opportunities (a potential deal with client). Opportunity is presented by the following (main) columns (properties): OpportunityId - to identify a separate Opportunity uniquely, CreatedDate - the date of the record creation, StageName - probability and the short stage description for a successful deal closing (100% Opportunity) Amount - (a current corrected version of) sum of the (potential) deal // ??? CloseDate - forecasted deal closing date Probability - the probability of success for this opportunity at its current stage other fields - properties of the Opportunity\n",
    "\n",
    "You have the history of interactions with clients (Opportunity is a potential deal).\n",
    "You should forecast the Probability of successful deal (Opportunity) closing.\n",
    "Speaking more precisely, to forecast most likely next Probability using the known previous history for this Opportunity (potential deal), namely Stage + Probability changes in previous time.\n",
    "Also you could use available properties of that Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380bf5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from datetime import datetime\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f08620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test_data_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf7b803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Unnamed: 0                            0\nId                                    0\nOpportunityId                         0\nCreatedById                           0\nCreatedDate                           0\nCreatedDateForInsert                  0\nStageName                             0\nAmount                                0\nProbability                           0\nPrevOpportunityStageUpdate            0\nValidThroughDate                    927\nSystemModstamp                        0\nIsDeleted                             0\nPrevAmount                         3843\nPrevCloseDate                      3741\nAccountId                             0\nRecordTypeId                        216\nStageSortOrder                        0\nType                                  0\nLeadSource                          681\nCampaignId                            0\nOwnerId                               0\nTerritory2Id                          0\nFiscalYear                            0\nFiscalQuarter                         0\nContactId                             0\nPartnerAccountId                    110\nNumber_of_Seats__c                  477\nSCC_Interest__c                       0\nCompany_s_CRM_Type__c              1045\nDeal_type__c                       1107\nNeeds__c                           2998\nType_of_Users__c                   4455\nTAM__c                              274\nSecondary_CSM__c                    300\nSource_Type__c                     1350\nOriginal_Lead_Source__c             972\nOriginal_Lead_Source_Details__c    1289\ndtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_count = df.isnull().sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a5874d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Id, OpportunityId, CreatedById, CreatedDate, CreatedDateForInsert, StageName, Amount, Probability, PrevOpportunityStageUpdate, ValidThroughDate, SystemModstamp, IsDeleted, PrevAmount, PrevCloseDate, AccountId, RecordTypeId, StageSortOrder, Type, LeadSource, CampaignId, OwnerId, Territory2Id, FiscalYear, FiscalQuarter, ContactId, PartnerAccountId, Number_of_Seats__c, SCC_Interest__c, Company_s_CRM_Type__c, Deal_type__c, Needs__c, Type_of_Users__c, TAM__c, Secondary_CSM__c, Source_Type__c, Original_Lead_Source__c, Original_Lead_Source_Details__c]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 38 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bohdan Potuzhnyi\\AppData\\Local\\Temp\\ipykernel_11368\\1553354875.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.StageName = data.StageName.apply(lambda x: '1' if '100' in x else '0')\n"
     ]
    }
   ],
   "source": [
    "data = df[df['StageName'].str.contains('100%') | df['StageName'].str.contains('0% prob')]\n",
    "\n",
    "print(data.loc[(data.OpportunityId == \"0061300001KubxZAAR\")])\n",
    "\n",
    "data.StageName = data.StageName.apply(lambda x: '1' if '100' in x else '0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37927834",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "OpportunityId\n0061300001KIkjMAAT     1\n0061B00001RMeAjQAL     1\n0061300001HidygAAB     1\n0061300001HjfPrAAJ     1\n0061300001FVMHDAA5     1\n                      ..\n0061B00001WwkEyQAJ    25\n0061300001NFy3aAAD    25\n0061B00001hqcoVQAQ    26\n0061B00001ZGR8PQAX    33\n0061B00001fgTQlQAM    47\nName: OpportunityId, Length: 803, dtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = data.groupby(\"OpportunityId\").OpportunityId.count()\n",
    "series.sort_values(inplace=True)\n",
    "series.head(3000)\n",
    "#opportunity_all = series[\"OpportunityId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "894415b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_opportunities = []\n",
    "for index, row in data.iterrows():\n",
    "    #print(row)\n",
    "    all_opportunities.append(row[\"OpportunityId\"])\n",
    "all_opportunities = np.unique(all_opportunities)\n",
    "#print(all_opportunities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97947a27",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### In this part for each opportunity from series DF i will create Array with all data that we\n",
    "# get from functions described lower\n",
    "# Firstly i will create array with all opportunities\n",
    "\n",
    "\n",
    "#now create DF and convert it to array for each op from all opportunities and then push them all\n",
    "# to one array\n",
    "all_opportunities_array = []\n",
    "final_probability_array = []\n",
    "for opportunity in all_opportunities:\n",
    "    opp_id = opportunity\n",
    "    specific_opportinity = data.loc[(data.OpportunityId == opp_id)]\n",
    "    #status = (specific_opportinity.loc[specific_opportinity.index[0], 'StageName'] == '1')\n",
    "    # 1-success\n",
    "    # 0-failure\n",
    "\n",
    "    #print(status)\n",
    "    specific_opportinity = specific_opportinity.sort_values(by=\"CreatedDateForInsert\", ascending=True)\n",
    "    #specific_opportinity\n",
    "\n",
    "    #print(specific_opportinity)\n",
    "    start_date = specific_opportinity.loc[specific_opportinity.index[0], \"CreatedDate\"]\n",
    "    start_date = datetime.strptime(start_date, '%m/%d/%Y %H:%M')\n",
    "\n",
    "    res_arr = []\n",
    "    arr = []\n",
    "    i = 0\n",
    "    for current in specific_opportinity[\"CreatedDateForInsert\"]:\n",
    "        current_date = datetime.strptime(current, '%Y-%m-%d %H:%M:%S')\n",
    "        res_arr.append(int((current_date - start_date).total_seconds()))\n",
    "        if i == 0:\n",
    "            arr.append(int((current_date - start_date).total_seconds()))\n",
    "            i+=1\n",
    "        else:\n",
    "            arr.append(int((current_date - start_date).total_seconds()) - arr[i-1])\n",
    "            i+=1\n",
    "\n",
    "    specific_opportinity = specific_opportinity.loc[:, [\"Amount\", \"Probability\"]]\n",
    "    specific_opportinity.insert(2, \"Duration\", res_arr)\n",
    "    specific_opportinity.insert(3, \"DeltaDuration\", arr)\n",
    "    final_probability = specific_opportinity.iloc[-1, specific_opportinity.columns.get_loc('Probability')]\n",
    "    final_probability_array.append(final_probability)\n",
    "    #print(final_probability)\n",
    "    opp_array = np.array(specific_opportinity)\n",
    "    all_opportunities_array.append(opp_array)\n",
    "\n",
    "###\n",
    "\n",
    "# opp_id = \"006a000000xiGkGAAU\"\n",
    "# specific_opportinity = data.loc[(data.OpportunityId == opp_id)]\n",
    "# print(specific_opportinity)\n",
    "# status = (specific_opportinity.loc[specific_opportinity.index[0], 'StageName'] == '1')\n",
    "# # 1-success\n",
    "# # 0-failure\n",
    "\n",
    "# #print(status)\n",
    "# specific_opportinity = specific_opportinity.sort_values(by=\"CreatedDateForInsert\", ascending=True)\n",
    "# #specific_opportinity\n",
    "\n",
    "# start_date = specific_opportinity.loc[specific_opportinity.index[0], \"CreatedDate\"]\n",
    "# start_date = datetime.strptime(start_date, '%m/%d/%Y %H:%M')\n",
    "\n",
    "# res_arr = []\n",
    "# arr = []\n",
    "# i = 0\n",
    "# for current in specific_opportinity[\"CreatedDateForInsert\"]:\n",
    "#     current_date = datetime.strptime(current, '%Y-%m-%d %H:%M:%S')\n",
    "#     res_arr.append(int((current_date - start_date).total_seconds()))\n",
    "#     if i == 0:\n",
    "#         arr.append(int((current_date - start_date).total_seconds()))\n",
    "#         i+=1\n",
    "#     else:\n",
    "#         arr.append(int((current_date - start_date).total_seconds()) - arr[i-1])\n",
    "#         i+=1\n",
    "    \n",
    "# specific_opportinity = specific_opportinity.loc[:, [\"CreatedById\", \"Amount\", \"Probability\"]]\n",
    "# specific_opportinity.insert(3, \"Duration\", res_arr)\n",
    "# specific_opportinity.insert(4, \"DeltaDuration\", arr)\n",
    "# final_probability = specific_opportinity.iloc[-1, specific_opportinity.columns.get_loc('Probability')]\n",
    "# print(final_probability)\n",
    "# opp_array = np.array(specific_opportinity)\n",
    "# print(opp_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65e17987",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print(all_opportunities_array)\n",
    "#final_probability_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "526f1490",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {\"<class 'numpy.int64'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m data_model \u001B[38;5;241m=\u001B[39m tensorflow\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[0;32m      2\u001B[0m     [\n\u001B[0;32m      3\u001B[0m         tensorflow\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDense(\u001B[38;5;241m128\u001B[39m),\n\u001B[0;32m      4\u001B[0m         tensorflow\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDense(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      5\u001B[0m     ]\n\u001B[0;32m      6\u001B[0m )\n\u001B[0;32m      8\u001B[0m data_model\u001B[38;5;241m.\u001B[39mcompile(loss \u001B[38;5;241m=\u001B[39m tensorflow\u001B[38;5;241m.\u001B[39mlosses\u001B[38;5;241m.\u001B[39mMeanSquaredError(),\n\u001B[0;32m      9\u001B[0m                    optimizer \u001B[38;5;241m=\u001B[39m tensorflow\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mAdam())\n\u001B[1;32m---> 11\u001B[0m \u001B[43mdata_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mall_opportunities_array\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinal_probability_array\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\dev_programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32mD:\\dev_programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:984\u001B[0m, in \u001B[0;36mselect_data_adapter\u001B[1;34m(x, y)\u001B[0m\n\u001B[0;32m    981\u001B[0m adapter_cls \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mcls\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m ALL_ADAPTER_CLS \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mcan_handle(x, y)]\n\u001B[0;32m    982\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m adapter_cls:\n\u001B[0;32m    983\u001B[0m   \u001B[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001B[39;00m\n\u001B[1;32m--> 984\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    985\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to find data adapter that can handle \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    986\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    987\u001B[0m           _type_name(x), _type_name(y)))\n\u001B[0;32m    988\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(adapter_cls) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    989\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    990\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData adapters should be mutually exclusive for \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    991\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhandling inputs. Found multiple adapters \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m to handle \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    992\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    993\u001B[0m           adapter_cls, _type_name(x), _type_name(y)))\n",
      "\u001B[1;31mValueError\u001B[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {\"<class 'numpy.int64'>\"})"
     ]
    }
   ],
   "source": [
    "data_model = tensorflow.keras.Sequential(\n",
    "    [\n",
    "        tensorflow.keras.layers.Dense(128),\n",
    "        tensorflow.keras.layers.Dense(1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_model.compile(loss = tensorflow.losses.MeanSquaredError(),\n",
    "                   optimizer = tensorflow.optimizers.Adam())\n",
    "\n",
    "data_model.fit(all_opportunities_array, final_probability_array, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf75c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "specific_opportinity.drop_duplicates(subset=['Duration'], inplace=True)\n",
    "specific_opportinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98528676",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(specific_opportinity.corr('pearson'), annot=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(x=\"Duration\", y=\"Probability\", data=specific_opportinity)\n",
    "#sns.scatterplot(x=\"Duration\", y=\"Probability\", data=specific_opportinity)\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=500, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec05020",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,7))\n",
    "X = specific_opportinity['Duration'].values[:,np.newaxis]\n",
    "# target data is array of shape (n,) \n",
    "y = specific_opportinity['Probability'].values\n",
    "\n",
    "regressor.fit(X, y)\n",
    "\n",
    "y_prediction_forest = regressor.predict(X)\n",
    "sns.scatterplot(x=specific_opportinity['Duration'], y = y)\n",
    "sns.lineplot(x=specific_opportinity['Duration'], y = y_prediction_forest)\n",
    "\n",
    "print('score -> ', regressor.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1994048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}